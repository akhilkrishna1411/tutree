Dimensionality reduction:
As data generation and collection keeps increasing, visualizing it and drawing inferences becomes more and more challenging
whenever we are having a 2-Dimensions datset or 3D we can easily visualize through scatter plot but what if we have more than 3D i.e if we are having 100 dimension data
then dimenionality reduction plays a major role.if our data lies in n-D space we try to convert into 2D or 3D
till now we are able what exactly dimensi0nality reduction is

1)** now let me know why dimensinality reduction is important
  1.1)*it rotates the data in such a way the reuslt is uncorrelated
  1.2)*It takes care of highly correlated features by removing them
  1.3)*training time will be much lesser i.e  training time  will be much faster
2)The set of techniques that can be employed for dimension reduction can be:  
2.1)principal Component Analysis
2.2)Independent Component Analysis
2.3)Methods Based on Projections
2.4)t-Distributed Stochastic Neighbor Embedding (t-SNE)

3)dimensionality reduction for linear data:
The most widely used techniques for dealing with linear data is proncipal component analysis.It divides the data into a set of components which try to explain
as much variance as possible.
4)dimensionality reduction for non-linear data:
This technique  works well when the data is non-linear. It works good for visualizations also

4)disadvantages: 
  4.1)classification in reduced space is lot worse than classification in original space
  4.2) Independent variables become less interpretable
5)finally am so glad to share a code from scratch as well as mathematics behind the alogorithm for one of the most commonly used dimensionality reduction technique
called principal component analysis(PCA):

1) representing the datapoints:
for example if i have features like [cricket football volleyball skate ] now am having four fetaures so my datapoint x(suffix i) belongs to R power(4) i.e R a real value number
2) data preprocessing:
before creating a model we need to preprocess our data
* column normalization:if a(i) is my data point then a(i)'is given by a(i)-a(min)/a(max)-a(min)
*value of a(i)' always lies between[0-1]
*why do we use column normalization:getting rid of scale
*without spoiling the data we are squeezing the data

